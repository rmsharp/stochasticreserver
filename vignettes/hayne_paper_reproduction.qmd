---
title: "Reproducing Hayne's Flexible Framework for Stochastic Reserving Models"
author: "R. Mark Sharp"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
    number-sections: true
vignette: >
  %\VignetteIndexEntry{Reproducing Hayne's Flexible Framework}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r setup, include=FALSE}
# Check if package is available (handles R CMD check case)
pkg_available <- requireNamespace("stochasticreserver", quietly = TRUE)

knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  eval = pkg_available
)

if (pkg_available) {
  library(stochasticreserver)
}
```

## Introduction

This vignette reproduces the key results from Roger Hayne's paper "A Flexible
Framework for Stochastic Reserving Models" published in *Variance* journal
[@hayne2012]. The paper is available at:

- [Variance Journal](https://variancejournal.org/article/120823)
- [CAS PDF](https://www.casact.org/sites/default/files/2021-08/Flexible-Framework-Hayne.pdf)

The `stochasticreserver` package implements all five reserving models described
in the paper using a unified maximum likelihood estimation framework.

## Theoretical Framework

### The Stochastic Model

Hayne's framework treats incremental average loss amounts as random variables
following a normal distribution. For accident year $i$ and development lag $j$:

$$A_{ij} \sim N(g_{ij}(\theta), \sigma^2_{ij})$$

where:

- $A_{ij}$ is the observed incremental average
- $g_{ij}(\theta)$ is the expected value function (model-specific)
- $\sigma^2_{ij}$ is the variance
- $\theta$ is the parameter vector

### Variance Structure

The variance follows a power model:

$$\sigma^2_{ij} = e^{\kappa - w_i} \cdot (g_{ij}(\theta)^2)^p$$

where:

- $\kappa$ is a proportionality constant
- $w_i = \log(d_i)$ with $d_i$ being the exposure count for year $i$
- $p$ is a power parameter (0 = constant variance, 0.5 = Poisson-like, 1 = constant CV)

### Maximum Likelihood Estimation

The log-likelihood function for the observed data is:

$$\ln L = -\frac{1}{2} \sum_{i,j} \left[ \ln(2\pi\sigma^2_{ij}) + \frac{(A_{ij} - g_{ij}(\theta))^2}{\sigma^2_{ij}} \right]$$

The negative log-likelihood is minimized to obtain parameter estimates.

### Gradient of the Objective Function

The gradient (first derivatives) is required for efficient optimization:

$$\frac{\partial(-\ln L)}{\partial\theta_k} = \sum_{i,j} \left[ \frac{\partial g_{ij}}{\partial\theta_k} \left( \frac{p}{g_{ij}} + \frac{g_{ij} - A_{ij}}{\sigma^2_{ij}} - \frac{p(A_{ij} - g_{ij})^2}{\sigma^2_{ij} g_{ij}} \right) \right]$$

### Hessian and Fisher Information

The Hessian matrix (second derivatives) provides the Fisher information:

$$I(\theta) = E\left[-\frac{\partial^2 \ln L}{\partial\theta_k \partial\theta_l}\right]$$

The inverse of the Fisher information matrix gives the variance-covariance
matrix for parameter estimates, enabling uncertainty quantification:

$$\text{Var}(\hat{\theta}) \approx I(\hat{\theta})^{-1}$$

## The Five Reserving Models

Each model specifies a different form for the expected value function
$g_{ij}(\theta)$.

### Chain Ladder Model

The Chain Ladder method assumes development follows a multiplicative pattern:

$$g_{ij}(\theta) = \alpha_i \cdot \beta_j$$

with constraints:

- $\sum_j \beta_j = 1$ (development factors sum to 1)
- $\alpha_i$ represents ultimate loss for year $i$

**Parameters:** $2n - 1$ where $n$ is the triangle size (row and column factors
minus one constraint).

### Cape Cod Model

The Cape Cod (Bornhuetter-Ferguson variant) method:

$$g_{ij}(\theta) = \alpha \cdot d_i \cdot \beta_j$$

where:

- $\alpha$ is a single expected loss ratio parameter
- $d_i$ is the exposure for year $i$
- $\beta_j$ is the development pattern

**Parameters:** $n + 1$ (one loss ratio plus $n$ development factors).

### Berquist-Sherman Model

The Berquist-Sherman incremental severity method includes a trend parameter:

$$g_{ij}(\theta) = e^{\tau \cdot i} \cdot \gamma_j$$

where:

- $\tau$ is the trend parameter
- $\gamma_j$ represents incremental severity by lag

**Parameters:** $n + 1$ (trend plus $n$ severity factors).

### Hoerl Curve Model

The Hoerl curve provides a smooth parametric form:

$$g_{ij}(\theta) = \alpha \cdot j^{\beta} \cdot e^{-\gamma j}$$

where:

- $\alpha$ is a scale parameter
- $\beta$ controls the shape (rise)
- $\gamma$ controls the decay rate

**Parameters:** 3 (highly parsimonious).

### Wright Model

The Wright model generalizes Hoerl with individual accident year levels:

$$g_{ij}(\theta) = \alpha_i \cdot j^{\beta} \cdot e^{-\gamma j}$$

where each accident year $i$ has its own level $\alpha_i$ but shares the shape
parameters.

**Parameters:** $n + 2$ (individual levels plus shared shape parameters).

## Data from the Paper

The package includes the development triangle from Table 1 of Hayne's paper:

```{r load-data}
# Load package data
data(B0, package = "stochasticreserver")
data(A0, package = "stochasticreserver")
data(dnom, package = "stochasticreserver")

size <- nrow(B0)
```

### Table 1: Development Triangle (Cumulative)

```{r table-1-triangle}
# Display the cumulative triangle
cumulative <- B0
for (j in 2:size) {
  cumulative[, j] <- rowSums(B0[, 1:j], na.rm = TRUE)
  cumulative[is.na(B0[, j]), j] <- NA
}

knitr::kable(
  round(cumulative, 2),
  caption = "Table 1: Cumulative Development Triangle",
  col.names = paste0("Lag ", 1:size)
)
```

### Incremental Averages

```{r incremental-averages}
knitr::kable(
  round(A0, 4),
  caption = "Incremental Average Amounts (A0)",
  col.names = paste0("Lag ", 1:size)
)
```

### Exposure Counts

```{r exposures}
exposure_df <- data.frame(
  `Accident Year` = 1:size,
  `Exposure (dnom)` = dnom
)
knitr::kable(exposure_df, caption = "Exposure Counts by Accident Year")
```

## Model Fitting

We now fit all five models using the package functions.

```{r setup-matrices}
# Set up required matrices
rowNum <- row(A0)
colNum <- col(A0)
logd <- log(matrix(dnom, size, size))

# Upper triangle mask
upper_triangle_mask <- (size - rowNum) >= colNum - 1

# Diagonal masks
msd <- (size - rowNum) == colNum - 1
msn <- (size - rowNum) == colNum - 2

# Amount paid to date
paid_to_date <- rowSums(B0 * msd, na.rm = TRUE)
```

### Fitting the Chain Ladder Model

```{r fit-chain}
chain_model <- chain(B0, paid_to_date, upper_triangle_mask)
g_chain <- chain_model$g_obj
a0_chain <- chain_model$a0

# Create objective function wrapper for optim
nll_chain <- function(a) {
  make_negative_log_likelihood(a, A0, dnom, g_chain)
}

# Optimize
fit_chain <- optim(
  par = c(a0_chain, 10, 1),
  fn = nll_chain,
  method = "BFGS",
  hessian = TRUE
)
```

### Fitting the Cape Cod Model

```{r fit-capecod}
capecod_model <- capecod(B0, paid_to_date, upper_triangle_mask)
g_capecod <- capecod_model$g_obj
a0_capecod <- capecod_model$a0

nll_capecod <- function(a) {
  make_negative_log_likelihood(a, A0, dnom, g_capecod)
}

fit_capecod <- optim(
  par = c(a0_capecod, 10, 1),
  fn = nll_capecod,
  method = "BFGS",
  hessian = TRUE
)
```

### Fitting the Berquist-Sherman Model

```{r fit-berquist}
berquist_model <- berquist(B0, paid_to_date, upper_triangle_mask)
g_berquist <- berquist_model$g_obj
a0_berquist <- berquist_model$a0

nll_berquist <- function(a) {
  make_negative_log_likelihood(a, A0, dnom, g_berquist)
}

fit_berquist <- optim(
  par = c(a0_berquist, 10, 1),
  fn = nll_berquist,
  method = "BFGS",
  hessian = TRUE
)
```

### Fitting the Hoerl Model

```{r fit-hoerl}
hoerl_model <- hoerl(B0, paid_to_date, upper_triangle_mask)
g_hoerl <- hoerl_model$g_obj
a0_hoerl <- hoerl_model$a0

nll_hoerl <- function(a) {
  make_negative_log_likelihood(a, A0, dnom, g_hoerl)
}

fit_hoerl <- optim(
  par = c(a0_hoerl, 10, 1),
  fn = nll_hoerl,
  method = "BFGS",
  hessian = TRUE
)
```

### Fitting the Wright Model

```{r fit-wright}
wright_model <- wright(B0, paid_to_date, upper_triangle_mask)
g_wright <- wright_model$g_obj
a0_wright <- wright_model$a0

nll_wright <- function(a) {
  make_negative_log_likelihood(a, A0, dnom, g_wright)
}

fit_wright <- optim(
  par = c(a0_wright, 10, 1),
  fn = nll_wright,
  method = "BFGS",
  hessian = TRUE
)
```

## Parameter Estimates

```{r parameter-table}
# Extract number of model parameters (excluding kappa and p)
n_params <- c(
  Chain = length(a0_chain),
  CapeCod = length(a0_capecod),
  Berquist = length(a0_berquist),
  Hoerl = length(a0_hoerl),
  Wright = length(a0_wright)
)

param_summary <- data.frame(
  Model = names(n_params),
  `Model Parameters` = n_params,
  `Total Parameters` = n_params + 2,
  `Neg Log-Likelihood` = c(
    fit_chain$value,
    fit_capecod$value,
    fit_berquist$value,
    fit_hoerl$value,
    fit_wright$value
  ),
  Converged = c(
    fit_chain$convergence == 0,
    fit_capecod$convergence == 0,
    fit_berquist$convergence == 0,
    fit_hoerl$convergence == 0,
    fit_wright$convergence == 0
  )
)

knitr::kable(
  param_summary,
  caption = "Parameter Summary by Model",
  digits = 2
)
```

## Reserve Estimates

```{r calculate-reserves}
# Lower triangle mask (future values to predict)
lower_mask <- !upper_triangle_mask

# Function to calculate reserve from fitted model
calculate_reserve <- function(g_func, params) {
  E <- g_func(params)
  sum(E * lower_mask, na.rm = TRUE)
}

# Extract model parameters (excluding kappa and p)
params_chain <- fit_chain$par[1:length(a0_chain)]
params_capecod <- fit_capecod$par[1:length(a0_capecod)]
params_berquist <- fit_berquist$par[1:length(a0_berquist)]
params_hoerl <- fit_hoerl$par[1:length(a0_hoerl)]
params_wright <- fit_wright$par[1:length(a0_wright)]

reserves <- data.frame(
  Model = c("Chain Ladder", "Cape Cod", "Berquist-Sherman", "Hoerl", "Wright"),
  Reserve = c(
    calculate_reserve(g_chain, params_chain),
    calculate_reserve(g_capecod, params_capecod),
    calculate_reserve(g_berquist, params_berquist),
    calculate_reserve(g_hoerl, params_hoerl),
    calculate_reserve(g_wright, params_wright)
  )
)

knitr::kable(
  reserves,
  caption = "Reserve Estimates by Model",
  digits = 2,
  format.args = list(big.mark = ",")
)
```

## Model Comparison

### Information Criteria

We compare models using AIC and BIC:

$$\text{AIC} = 2k + 2(-\ln L)$$
$$\text{BIC} = k \ln(n) + 2(-\ln L)$$

where $k$ is the number of parameters and $n$ is the number of observations.

```{r model-comparison}
n_obs <- sum(upper_triangle_mask)  # Number of observations

model_comparison <- data.frame(
  Model = c("Chain Ladder", "Cape Cod", "Berquist-Sherman", "Hoerl", "Wright"),
  Parameters = c(
    length(a0_chain) + 2,
    length(a0_capecod) + 2,
    length(a0_berquist) + 2,
    length(a0_hoerl) + 2,
    length(a0_wright) + 2
  ),
  NegLogLik = c(
    fit_chain$value,
    fit_capecod$value,
    fit_berquist$value,
    fit_hoerl$value,
    fit_wright$value
  )
)

model_comparison$AIC <- 2 * model_comparison$Parameters +
  2 * model_comparison$NegLogLik
model_comparison$BIC <- model_comparison$Parameters * log(n_obs) +
  2 * model_comparison$NegLogLik

# Rank by AIC
model_comparison <- model_comparison[order(model_comparison$AIC), ]
model_comparison$AIC_Rank <- 1:nrow(model_comparison)

knitr::kable(
  model_comparison,
  caption = "Model Comparison using Information Criteria",
  digits = 2,
  row.names = FALSE
)
```

### Visual Comparison of Reserves

```{r fig-reserve-comparison}
#| fig-cap: "Reserve Estimates by Model"
#| fig-height: 5

# Handle potential NA/Inf values
reserve_vals <- reserves$Reserve
reserve_vals[!is.finite(reserve_vals)] <- 0

barplot(
  reserve_vals,
  names.arg = c("Chain", "CapeCod", "Berquist", "Hoerl", "Wright"),
  col = c("steelblue", "darkgreen", "darkorange", "purple", "darkred"),
  main = "Reserve Estimates by Model",
  ylab = "Reserve Amount",
  las = 1
)
if (any(is.finite(reserves$Reserve))) {
  abline(h = mean(reserves$Reserve, na.rm = TRUE), lty = 2, col = "gray40")
  legend("topright", "Mean Reserve", lty = 2, col = "gray40", bty = "n")
}
```

## Residual Diagnostics

### Chain Ladder Residuals

```{r fig-residuals-chain}
#| fig-cap: "Chain Ladder Model Residuals"
#| fig-height: 8

# Calculate fitted values and residuals
E_chain <- g_chain(params_chain)
residuals_chain <- (A0 - E_chain) * upper_triangle_mask
residuals_chain[!upper_triangle_mask] <- NA

# Standardized residuals
kappa <- fit_chain$par[length(a0_chain) + 1]
p <- fit_chain$par[length(a0_chain) + 2]
var_chain <- exp(kappa - logd) * (pmax(E_chain^2, 1e-10))^p
std_resid_chain <- residuals_chain / sqrt(pmax(var_chain, 1e-10))
std_resid_chain[!is.finite(std_resid_chain)] <- NA

par(mfrow = c(2, 2))

# Q-Q plot
std_resid_vec <- as.vector(std_resid_chain)
std_resid_vec <- std_resid_vec[is.finite(std_resid_vec)]
qqnorm(std_resid_vec, main = "Q-Q Plot: Chain Ladder")
qqline(std_resid_vec, col = "red")

# Residuals vs Fitted
fitted_vals <- as.vector(E_chain[upper_triangle_mask])
resid_vals <- as.vector(std_resid_chain[upper_triangle_mask])
valid_idx <- is.finite(fitted_vals) & is.finite(resid_vals)
plot(
  fitted_vals[valid_idx],
  resid_vals[valid_idx],
  xlab = "Fitted Values",
  ylab = "Standardized Residuals",
  main = "Residuals vs Fitted",
  pch = 19,
  col = "steelblue"
)
abline(h = 0, lty = 2, col = "red")

# Residuals by Development Lag
boxplot(
  std_resid_chain,
  names = 1:size,
  xlab = "Development Lag",
  ylab = "Standardized Residuals",
  main = "Residuals by Development Lag",
  col = "lightblue"
)
abline(h = 0, lty = 2, col = "red")

# Residuals by Calendar Year
cal_year <- row(A0) + col(A0) - 1
resid_by_cal <- tapply(
  as.vector(std_resid_chain),
  as.vector(cal_year),
  mean,
  na.rm = TRUE
)
barplot(
  resid_by_cal,
  xlab = "Calendar Year",
  ylab = "Mean Standardized Residual",
  main = "Residuals by Calendar Year",
  col = "lightgreen"
)
abline(h = 0, lty = 2, col = "red")

par(mfrow = c(1, 1))
```

## Simulation and Uncertainty

### Monte Carlo Simulation

We use the Fisher information matrix to simulate parameter uncertainty and
estimate the distribution of reserves.

```{r simulation}
set.seed(42)
n_sim <- 5000

# Use Chain Ladder for simulation example
params_full <- fit_chain$par
hessian_chain <- fit_chain$hessian

# Variance-covariance matrix from Hessian (Fisher information approximation)
vcov_chain <- tryCatch(
  solve(hessian_chain),
  error = function(e) {
    # If Hessian is singular, use pseudo-inverse
    MASS::ginv(hessian_chain)
  }
)

# Ensure positive definiteness
vcov_chain <- (vcov_chain + t(vcov_chain)) / 2
eig <- eigen(vcov_chain)
eig$values[eig$values < 0] <- 1e-6
vcov_chain <- eig$vectors %*% diag(eig$values) %*% t(eig$vectors)

# Simulate parameters
sim_params <- MASS::mvrnorm(n_sim, params_full, vcov_chain)

# Calculate reserves for each simulation
sim_reserves <- numeric(n_sim)
for (i in 1:n_sim) {
  model_params <- sim_params[i, 1:length(a0_chain)]
  sim_reserves[i] <- calculate_reserve(g_chain, model_params)
}
```

### Distribution of Reserve Estimates

```{r fig-reserve-distribution}
#| fig-cap: "Distribution of Simulated Reserves (Chain Ladder)"
#| fig-height: 5

hist(
  sim_reserves,
  breaks = 50,
  col = "steelblue",
  border = "white",
  main = "Distribution of Reserve Estimates",
  xlab = "Reserve Amount",
  freq = FALSE
)

# Add density curve
lines(density(sim_reserves), col = "darkred", lwd = 2)

# Add confidence interval lines
ci <- quantile(sim_reserves, c(0.025, 0.5, 0.975))
abline(v = ci, col = c("orange", "red", "orange"), lty = c(2, 1, 2), lwd = 2)
legend(
  "topright",
  c("Density", "Median", "95% CI"),
  col = c("darkred", "red", "orange"),
  lty = c(1, 1, 2),
  lwd = 2,
  bty = "n"
)
```

### Summary Statistics

```{r simulation-summary}
sim_summary <- data.frame(
  Statistic = c(
    "Mean Reserve",
    "Median Reserve",
    "Standard Error",
    "Coefficient of Variation",
    "2.5% Quantile",
    "97.5% Quantile",
    "95% Confidence Interval Width"
  ),
  Value = c(
    mean(sim_reserves),
    median(sim_reserves),
    sd(sim_reserves),
    sd(sim_reserves) / mean(sim_reserves),
    quantile(sim_reserves, 0.025),
    quantile(sim_reserves, 0.975),
    diff(quantile(sim_reserves, c(0.025, 0.975)))
  )
)

knitr::kable(
  sim_summary,
  caption = "Monte Carlo Simulation Summary (Chain Ladder, 5,000 iterations)",
  digits = 2,
  format.args = list(big.mark = ",")
)
```

## Conclusion

This vignette has reproduced the key elements of Hayne's flexible framework:

1. **Theoretical Foundation**: The unified likelihood-based approach enables
   consistent parameter estimation and uncertainty quantification across all
   five reserving methods.

2. **Model Comparison**: Information criteria (AIC/BIC) provide objective
   model selection guidance.

3. **Uncertainty Quantification**: The Fisher information matrix and Monte
   Carlo simulation enable probabilistic reserve estimates with confidence
   intervals.

4. **Practical Implementation**: The `stochasticreserver` package provides
   ready-to-use functions for all five models with gradient and Hessian
   computations for efficient optimization.

## References

::: {#refs}
:::
